# ğŸ“Œ AnÃ¡lise de Componentes Principais (PCA)

## ğŸ¯ Objetivos da Aula
Nesta aula, vamos abordar os seguintes tÃ³picos:
- O conceito de **reduÃ§Ã£o de dimensionalidade** e sua importÃ¢ncia em Machine Learning.
- A **AnÃ¡lise de Componentes Principais (PCA)** como mÃ©todo estatÃ­stico para simplificar conjuntos de dados.
- Fundamentos matemÃ¡ticos: **variÃ¢ncia, covariÃ¢ncia, autovalores e autovetores**.
- ImplementaÃ§Ã£o prÃ¡tica do **PCA em Python**.

---

## ğŸ” 1. O que Ã© PCA e por que usÃ¡-lo?
A **AnÃ¡lise de Componentes Principais (PCA)** Ã© uma tÃ©cnica estatÃ­stica usada para **reduzir a dimensionalidade dos dados**, mantendo a maior quantidade possÃ­vel de variÃ¢ncia.

### âœ… **MotivaÃ§Ãµes para o uso do PCA:**
1. **ReduÃ§Ã£o da dimensionalidade** â†’ Facilita a visualizaÃ§Ã£o e anÃ¡lise de dados.
2. **EliminaÃ§Ã£o de multicolinearidade** â†’ Remove redundÃ¢ncias entre variÃ¡veis correlacionadas.
3. **Melhoria na performance de modelos** â†’ Reduz o risco de sobreajuste.
4. **VisualizaÃ§Ã£o de dados complexos** â†’ Permite projetar dados multidimensionais em um espaÃ§o menor.

---

## ğŸ“Š 2. Fundamentos MatemÃ¡ticos do PCA
### ğŸ“Œ **VariÃ¢ncia**
A **variÃ¢ncia** mede a dispersÃ£o dos dados em relaÃ§Ã£o Ã  mÃ©dia:

```math
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
```

Onde:
- $$\ x_i \$$ sÃ£o os valores individuais da variÃ¡vel.
- $$\ \bar{x} \$$ Ã© a mÃ©dia dos valores.

### ğŸ“Œ **CovariÃ¢ncia**
A **covariÃ¢ncia** mede como duas variÃ¡veis variam juntas:

```math
	\text{cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
```

Uma **matriz de covariÃ¢ncia** captura a relaÃ§Ã£o entre mÃºltiplas variÃ¡veis:

```math
C = 
\begin{bmatrix}
	\text{Var}(X_1) 	& 	\text{Cov}(X_1, X_2) 	& \dots \\
	\text{Cov}(X_2, X_1) 	& 	\text{Var}(X_2) 	& \dots \\
	\dots 			& 	\dots 			& \ddots
\end{bmatrix}
```

### ğŸ“Œ **Autovalores e Autovetores**
Os **autovalores** ($$\\lambda \$$) e **autovetores** ($$\ v \$$) de uma matriz $$\ A \$$ satisfazem:
```math
A v = \lambda v
```

No contexto do PCA:
- **Autovetores** representam as **direÃ§Ãµes principais** dos dados.
- **Autovalores** indicam **quanta variÃ¢ncia** cada direÃ§Ã£o retÃ©m.

---

## ğŸ”¬ 3. Passos para Implementar o PCA
O PCA Ã© executado seguindo os passos:

1ï¸âƒ£ **Padronizar os dados** (mÃ©dia 0 e variÃ¢ncia 1).  
2ï¸âƒ£ **Calcular a matriz de covariÃ¢ncia** entre as variÃ¡veis.  
3ï¸âƒ£ **Encontrar os autovalores e autovetores** da matriz de covariÃ¢ncia.  
4ï¸âƒ£ **Ordenar os autovalores em ordem decrescente** e selecionar os principais.  
5ï¸âƒ£ **Projetar os dados no novo espaÃ§o** formado pelos componentes principais.

---

## ğŸ›  4. ImplementaÃ§Ã£o PrÃ¡tica do PCA em Python
Aqui estÃ¡ um exemplo de como implementar o PCA manualmente:

```python
import numpy as np
import pandas as pd

# Exemplo de dados
data = {
    'VariÃ¡vel_1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],
    'VariÃ¡vel_2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9]
}

df = pd.DataFrame(data)

# PadronizaÃ§Ã£o dos dados
df_padronizado = (df - df.mean()) / df.std()

# CÃ¡lculo da matriz de covariÃ¢ncia
matriz_covariancia = np.cov(df_padronizado.T)

# CÃ¡lculo dos autovalores e autovetores
autovalores, autovetores = np.linalg.eig(matriz_covariancia)

# OrdenaÃ§Ã£o dos autovalores e autovetores
pares_autovalores = [(autovalores[i], autovetores[:, i]) for i in range(len(autovalores))]
pares_autovalores.sort(key=lambda x: x[0], reverse=True)

# TransformaÃ§Ã£o dos dados
matriz_projecao = np.column_stack([pares_autovalores[i][1] for i in range(len(autovalores))])
dados_transformados = np.dot(df_padronizado, matriz_projecao)

# Criar DataFrame com os componentes principais
df_pca = pd.DataFrame(dados_transformados, columns=['Componente_1', 'Componente_2'])
print(df_pca)
```

ğŸ“Œ **Notebook:** [notebooks/principal_components_analysis.ipynb](notebooks/principal_components_analysis.ipynb)

---

## ğŸ“š 5. Recursos Recomendados
ğŸ“– **Livros:**
- â€œMathematics for Machine Learningâ€ â€“ Marc Peter Deisenroth.
- â€œDeep Learningâ€ â€“ Ian Goodfellow.

ğŸ¥ **Cursos Online:**
- [Coursera - Mathematics for Machine Learning](https://www.coursera.org/specializations/mathematics-machine-learning)
- [FastAI - Introduction to ML](https://course.fast.ai/)

---

## ğŸš€ 6. PrÃ³ximos Passos
Na prÃ³xima aula, exploraremos como integrar o **sklearn** ao PCA e utilizar a tÃ©cnica para **prÃ©-processamento de dados** antes de treinar redes neurais!

---
ğŸ“ Autor: **Prof. Erick Toshio Yamamoto**
ğŸ“… Data: 06/03/2025
ğŸ“Œ Disciplina: Redes Neurais e Deep Learning
