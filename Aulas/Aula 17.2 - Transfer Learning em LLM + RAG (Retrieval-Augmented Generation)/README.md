# Aula 17.2: Transfer Learning em LLM + RAG (Retrieval-Augmented Generation)

## üéØ Objetivos

* Entender os princ√≠pios de **Transfer Learning** em **LLMs**, por que ele reduz custo e dados.
* Implementar **fine-tuning eficiente** (LoRA/PEFT) em um modelo **text-to-text** (FLAN-T5).
* Compreender o **paradigma RAG** e quando utiliz√°-lo.
* Construir um pipeline **RAG**: **embeddings ‚Üí √≠ndice vetorial (FAISS) ‚Üí recupera√ß√£o ‚Üí gera√ß√£o**.

---

## üìò Parte 1 ‚Äî Teoria: Transfer Learning em LLMs

LLMs (Large Language Models) s√£o pr√©-treinados em corpora massivos para aprender padr√µes gerais de linguagem. **Transfer Learning** reutiliza esse conhecimento base e o **adapta** a uma tarefa espec√≠fica (ex.: QA jur√≠dico, sumariza√ß√£o cient√≠fica) com **poucos dados e menos tempo**. Duas abordagens comuns:

1. **Fine-tuning completo**: atualiza todos os par√¢metros do modelo (caro em mem√≥ria/tempo).
2. **Fine-tuning eficiente (parameter-efficient)**: congela a maior parte do modelo e treina **camadas adicionais leves** (ex.: **LoRA**, **Adapters**, **Prompt Tuning**). Na pr√°tica atual, **LoRA/PEFT** √© padr√£o por reduzir ordens de grandeza de custo e viabilizar treino em GPU √∫nica.

Intui√ß√£o: pense no LLM como um **poliglota experiente**. Em vez de reaprender ‚Äúo idioma do zero‚Äù, voc√™ d√° **aulas particulares** focadas na **g√≠ria** e **jarg√£o** da sua √°rea (dom√≠nio), ajustando apenas ‚Äúpoucos neur√¥nios‚Äù ‚Äî isso √© o LoRA.

**Leituras √∫teis:**

* Houlsby et al., *Parameter-Efficient Transfer Learning for NLP*, ICML 2019 (Adapters).
* Hu et al., *LoRA: Low-Rank Adaptation of Large Language Models*, ICLR 2022.
* Raffel et al., *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)*, JMLR 2020.

---

## üíª Parte 2 ‚Äî Fine-tuning eficiente (LoRA/PEFT) com FLAN-T5

### Vis√£o

Vamos adaptar **FLAN-T5-base** em um subtarefa de **QA estilo instru√ß√£o** (entrada: *‚ÄúContexto ‚Ä¶ Pergunta ‚Ä¶ Responda ‚Ä¶‚Äù*). Usaremos **PEFT** (Hugging Face) para aplicar **LoRA** √†s camadas de aten√ß√£o. O dataset de exemplo ser√° simplificado para fins did√°ticos (voc√™ pode trocar por SQuAD, TyDiQA, ou conjunto propriet√°rio).

### Depend√™ncias

```bash
pip install transformers datasets peft accelerate sentencepiece
```

### C√≥digo ‚Äî carregamento do modelo e prepara√ß√£o

```python
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType

# 1) Base: FLAN-T5
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 2) Configura√ß√£o LoRA (PEFT)
peft_cfg = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=8,              # rank do LoRA (trade-off custo x qualidade)
    lora_alpha=16,    # escala
    lora_dropout=0.05 # regulariza√ß√£o
)
model = get_peft_model(base_model, peft_cfg)

# 3) Dataset de exemplo (substitua por SQuAD ou seu corpus)
data = load_dataset("squad", split={"train":"train[:2%]","validation":"validation[:2%]"})
# Fun√ß√£o de formata√ß√£o: contexto + pergunta -> alvo = resposta curta
def format_example(ex):
    prompt = f"Contexto: {ex['context']}\nPergunta: {ex['question']}\nResponda de forma objetiva:"
    # usa a primeira resposta dispon√≠vel
    answer = ex["answers"]["text"][0] if len(ex["answers"]["text"])>0 else ""
    return {"prompt": prompt, "answer": answer}

proc_train = data["train"].map(format_example)
proc_val   = data["validation"].map(format_example)

# 4) Tokeniza√ß√£o
max_src, max_tgt = 512, 64
def tokenize(batch):
    model_inputs = tokenizer(batch["prompt"], max_length=max_src, truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(batch["answer"], max_length=max_tgt, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tok_train = proc_train.map(tokenize, batched=True, remove_columns=proc_train.column_names)
tok_val   = proc_val.map(tokenize, batched=True, remove_columns=proc_val.column_names)

collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

**Explica√ß√£o:** aplicamos **LoRA** nas camadas de aten√ß√£o do T5; congelamos o restante. O *dataset* foi minimizado para rodar r√°pido em aula. Em produ√ß√£o, use todo o conjunto e valide bem.

### Treinando apenas os par√¢metros LoRA

```python
args = TrainingArguments(
    output_dir="flan_t5_lora_qa",
    learning_rate=2e-4,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy="epoch",
    logging_steps=50,
    save_strategy="epoch",
    fp16=True
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tok_train,
    eval_dataset=tok_val,
    data_collator=collator,
    tokenizer=tokenizer
)

trainer.train()
```

### Infer√™ncia (p√≥s-treino)

```python
def generate_answer(context, question, max_new_tokens=64):
    prompt = f"Contexto: {context}\nPergunta: {question}\nResponda de forma objetiva:"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Exemplo
ctx = "A retropropaga√ß√£o ajusta os pesos de uma rede neural minimizando o erro."
q   = "O que √© retropropaga√ß√£o?"
print(generate_answer(ctx, q))
```

---

## üìò Parte 3 ‚Äî Teoria: RAG (Retrieval-Augmented Generation)

**Problema:** LLMs n√£o ‚Äúsabem tudo‚Äù do seu dom√≠nio e podem alucinar. **RAG** combina **recupera√ß√£o de conhecimento** (embeddings + busca sem√¢ntica) com **gera√ß√£o condicionada**. Fluxo:

1. **Embeddings**: converter documentos em vetores (Sentence-Transformers).
2. **√çndice**: FAISS ou similar.
3. **Consulta**: dado o *prompt*, recuperar **top-k** trechos relevantes.
4. **Gera√ß√£o**: concatenar o contexto recuperado ao *prompt* e pedir a resposta ao LLM (o seu **modelo adaptado via LoRA**, por exemplo).

Analogia: o LLM √© um **redator** talentoso; o RAG √© a **biblioteca** que voc√™ consulta antes de escrever. Juntos: respostas **contextualizadas, atualizadas e audit√°veis**.

**Leituras (n√£o-Wikipedia):**

* Lewis et al., *Retrieval-Augmented Generation for Knowledge-Intensive NLP*, NeurIPS 2020.
* Karpukhin et al., *Dense Passage Retrieval (DPR)*, EMNLP 2020.

---

## üíª Parte 4 ‚Äî Implementa√ß√£o RAG (FAISS + Embeddings + FLAN-T5)

### Depend√™ncias

```bash
pip install sentence-transformers faiss-cpu
```

### Passo 1 ‚Äî Criar base de conhecimento e embeddings

```python
from sentence_transformers import SentenceTransformer
import numpy as np, faiss

# 1) Documentos (troque por seus PDFs/HTML j√° chunkados)
docs = [
    "Gradiente descendente √© um m√©todo de otimiza√ß√£o para minimizar fun√ß√µes.",
    "A fun√ß√£o ReLU √© muito usada em redes profundas por mitigar satura√ß√£o.",
    "Batch normalization estabiliza a distribui√ß√£o das ativa√ß√µes e acelera o treino.",
    "A regulariza√ß√£o L2 penaliza pesos grandes e ajuda a reduzir overfitting."
]

embedder = SentenceTransformer("all-MiniLM-L6-v2")
doc_vecs = embedder.encode(docs, convert_to_numpy=True, normalize_embeddings=True)

dim = doc_vecs.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(doc_vecs)
```

**Explica√ß√£o:** usamos um **encoder** de senten√ßas para capturar sem√¢ntica. **Normalizamos** para que o **produto interno ‚âà cosseno**.

### Passo 2 ‚Äî Recuperar contexto para uma pergunta

```python
def retrieve(question, k=2):
    qv = embedder.encode([question], convert_to_numpy=True, normalize_embeddings=True)
    D, I = index.search(qv, k)
    ctx = "\n".join([docs[i] for i in I[0]])
    return ctx, (D[0], I[0])

question = "Como reduzir overfitting em redes neurais?"
context, meta = retrieve(question, k=2)
print("Contexto recuperado:\n", context)
```

### Passo 3 ‚Äî Gera√ß√£o condicionada ao contexto (RAG ‚Äúsimples‚Äù)

Use **o seu modelo LoRA** (adaptado no in√≠cio) para responder **com base no contexto recuperado**.

```python
def rag_answer(question, max_new_tokens=96):
    context, _ = retrieve(question, k=2)
    prompt = (
        "Use apenas o contexto dado para responder com objetividade.\n"
        f"Contexto:\n{context}\n\nPergunta: {question}\nResposta:"
    )
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
    return tokenizer.decode(outputs[0], skip_special_tokens=True), context

ans, used = rag_answer("Como reduzir overfitting em redes neurais?")
print("Resposta:", ans)
print("\n[Contexto usado]\n", used)
```

**Explica√ß√£o:** essa vers√£o ‚Äúcl√°ssica‚Äù do RAG concatena top-k contextos ao *prompt*. Em produ√ß√£o, voc√™ pode:

* Resumir os contextos (re-rank, compress√£o).
* Guardar **metadados** (origem/p√°gina/URL).
* Trocar FAISS Flat por **IVF/HNSW** para escalar.

---

## üß™ Atividade em Sala

1. Substitua o mini-dataset por **SQuAD** (ou base pr√≥pria PT-BR).
2. Treine **LoRA** por 1‚Äì2 √©pocas (subset) e salve os adaptadores.
3. Construa uma **base vetorial** com **10‚Äì20 textos** do seu dom√≠nio (ou PDF chunkado).
4. Compare: **(a) sem RAG** vs. **(b) com RAG** (mesmo modelo), usando 5 perguntas do tema.
5. Registre acertos, exemplos e **quando o RAG ajudou**.

---

## üß† Desafio para Casa

* Implementar **re-rank** (ex.: `cross-encoder` para reordenar top-k).
* Medir **lat√™ncia**: tempo da recupera√ß√£o + gera√ß√£o.
* Adicionar **cita√ß√µes**: ao responder, listar as fontes/trechos usados (IDs dos chunks).
* (Opcional) Interface **Gradio/Streamlit** com campo de upload de PDF ‚Üí *embed* ‚Üí RAG.

---

## ‚úÖ Conclus√µes

* **Transfer Learning** com **LoRA/PEFT** torna o ajuste de LLMs **vi√°vel** com poucos recursos.
* **RAG** complementa o LLM com **conhecimento atualizado**, reduz alucina√ß√µes e d√° **tra√ßabilidade** (voc√™ sabe de onde veio a resposta).
* A combina√ß√£o **LoRA + RAG** √© hoje um padr√£o de engenharia para chatbots de dom√≠nio.
