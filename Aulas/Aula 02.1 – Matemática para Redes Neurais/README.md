# ğŸ“Œ Fundamentos MatemÃ¡ticos para Redes Neurais

## ğŸ¯ Objetivos da Aula
Nesta aula, vamos abordar os seguintes tÃ³picos:
- Conceitos fundamentais de **Ãlgebra Linear** para Redes Neurais.
- RevisÃ£o de **CÃ¡lculo Diferencial** e sua aplicaÃ§Ã£o no aprendizado de mÃ¡quinas.
- IntroduÃ§Ã£o ao **Gradiente Descendente**, utilizado para otimizaÃ§Ã£o de redes neurais.

---

## ğŸ”¢ 1. Ãlgebra Linear para Redes Neurais
A **Ãlgebra Linear** Ã© essencial para o funcionamento das redes neurais, pois permite manipular grandes conjuntos de dados de maneira eficiente.

### ğŸŸ¢ **Vetores**
Os vetores representam magnitudes e direÃ§Ãµes no espaÃ§o. Um vetor **v** pode ser escrito como:

$$\ v = \begin{bmatrix} v_1 \ v_2 \dots \ v_n \end{bmatrix} \$$

### ğŸ”µ **Matrizes**
As matrizes armazenam dados organizados em **linhas e colunas**. Exemplo:

$$\ A = \begin{bmatrix} a_{11} & a_{12} \\ 
a_{21} & a_{22} \end{bmatrix} \$$

### âœï¸ **OperaÃ§Ãµes Matriciais**
1ï¸âƒ£ **Soma de Matrizes**  
$$
C = A + B
$$

2ï¸âƒ£ **MultiplicaÃ§Ã£o Escalar**  
\[
\lambda A = \begin{bmatrix} \lambda a_{11} & \lambda a_{12} \ \lambda a_{21} & \lambda a_{22} \end{bmatrix}
\]

3ï¸âƒ£ **Produto Matricial**  
Se \( A \) Ã© uma matriz \( m 	imes n \) e \( B \) Ã© uma matriz \( n 	imes p \), entÃ£o o produto \( C = A \cdot B \) resulta em uma matriz \( m 	imes p \).

4ï¸âƒ£ **TransposiÃ§Ã£o de Matriz**  
A transposiÃ§Ã£o troca as linhas e colunas de uma matriz:  
\[
A^T = \begin{bmatrix} a_{11} & a_{21} \ a_{12} & a_{22} \end{bmatrix}
\]

---

## ğŸ“ˆ 2. CÃ¡lculo Diferencial para Redes Neurais
O **CÃ¡lculo Diferencial** permite entender como pequenas mudanÃ§as nas entradas afetam a saÃ­da da rede neural.

### ğŸŸ¢ **Derivadas**
A derivada mede a **taxa de variaÃ§Ã£o** de uma funÃ§Ã£o. Exemplo:

Seja a funÃ§Ã£o:
\[
f(x) = x^2 + 3x + 2
\]
Sua derivada Ã©:
\[
f'(x) = 2x + 3
\]

### ğŸ”µ **Derivadas Parciais**
Usadas para funÃ§Ãµes com mÃºltiplas variÃ¡veis. Exemplo:

Seja a funÃ§Ã£o:
\[
f(x, y) = x^2 + 3xy + y^2
\]
A derivada parcial em relaÃ§Ã£o a \( x \) Ã©:
\[
rac{\partial f}{\partial x} = 2x + 3y
\]

---

## ğŸ”¥ 3. Gradiente Descendente e FunÃ§Ã£o de Custo
O **Gradiente Descendente** Ã© um dos algoritmos mais importantes para otimizar redes neurais. Ele ajusta os pesos do modelo de maneira iterativa para minimizar o erro.

### ğŸ“Œ **FunÃ§Ãµes de Custo**
A **funÃ§Ã£o de custo** mede a diferenÃ§a entre a saÃ­da prevista e o valor real. Exemplos:

1ï¸âƒ£ **Erro QuadrÃ¡tico MÃ©dio (MSE)**  
\[
MSE = rac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

2ï¸âƒ£ **Erro Absoluto MÃ©dio (MAE)**  
\[
MAE = rac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]

3ï¸âƒ£ **Entropia Cruzada (Cross-Entropy Loss)**  
\[
L = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
\]

### âš¡ **FÃ³rmula do Gradiente Descendente**
\[
w = w -  lpha 
abla L(w)
\]
Onde:
- \(  lpha \) Ã© a **taxa de aprendizado**.
- \( 
abla L(w) \) Ã© o **gradiente da funÃ§Ã£o de custo**.

---

## ğŸ›  4. ImplementaÃ§Ã£o PrÃ¡tica
ğŸ“Œ No arquivo **Jupyter Notebook** anexado, temos exemplos prÃ¡ticos de:
- Soma e multiplicaÃ§Ã£o de matrizes em Python.
- CÃ¡lculo de derivadas e derivadas parciais.
- ImplementaÃ§Ã£o do Gradiente Descendente para otimizaÃ§Ã£o de um modelo.

ğŸ”— **Notebook:** [notebooks/matematica_redes_neurais.ipynb](notebooks/matematica_redes_neurais.ipynb)

---

## ğŸ“š 5. Recursos Recomendados
ğŸ“– **Livros:**
- â€œMathematics for Machine Learningâ€ â€“ Marc Peter Deisenroth.
- â€œDeep Learningâ€ â€“ Ian Goodfellow.

ğŸ¥ **Cursos Online:**
- [Coursera - Mathematics for Machine Learning](https://www.coursera.org/specializations/mathematics-machine-learning)

---

## ğŸš€ 6. PrÃ³ximos Passos
Na prÃ³xima aula, exploraremos **AnÃ¡lise ExploratÃ³ria de Dados (EDA) e PrÃ©-processamento** para preparar os dados antes de treinar modelos de redes neurais!

