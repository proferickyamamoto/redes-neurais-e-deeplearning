# üß† √öltima Aula 11.1: Otimizadores Avan√ßados e Ajuste de Hiperpar√¢metros em Redes Neurais

## üéØ Objetivo

Encerrar o semestre com uma aula dedicada ao refinamento e melhoria de redes neurais, abordando **otimizadores modernos**, t√©cnicas de **tuning de hiperpar√¢metros** e ferramentas automatizadas para busca de melhores modelos.

---

## üìö Conte√∫do Program√°tico

### 1. Otimizadores Avan√ßados

| Otimizador   | Caracter√≠stica Principal                                                 |
| ------------ | ------------------------------------------------------------------------ |
| **SGD**      | Gradiente simples com taxa fixa                                          |
| **Momentum** | Adiciona "in√©rcia" √† atualiza√ß√£o                                         |
| **RMSProp**  | Ajusta a taxa de aprendizado com base na m√©dia quadr√°tica dos gradientes |
| **Adam**     | Combina Momentum + RMSProp; muito usado em deep learning                 |

### Exemplo com Keras:

```python
from keras.optimizers import Adam
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
```

---

### 2. Hiperpar√¢metros em Redes Neurais

* N√∫mero de neur√¥nios nas camadas
* N√∫mero de camadas ocultas
* Taxa de aprendizado (learning rate)
* Fun√ß√µes de ativa√ß√£o
* Tamanho do batch e n√∫mero de √©pocas

### T√©cnicas de Ajuste

* **Grid Search**
* **Random Search**
* **Bayesian Optimization** (ex: Optuna, Hyperopt)

---

## üîß Ferramentas de Tuning Automatizado

### Exemplo com GridSearchCV (sklearn):

```python
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier

param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50)],
    'alpha': [0.0001, 0.01],
    'learning_rate_init': [0.001, 0.01]
}

grid = GridSearchCV(MLPClassifier(max_iter=500), param_grid, cv=3)
grid.fit(X_train, y_train)
print("Melhores par√¢metros:", grid.best_params_)
```

---

## üß™ Atividade em Sala

**T√≠tulo:** Encontrando a melhor rede

### Instru√ß√µes:

1. Escolha um dataset conhecido (OpenML, sklearn).
2. Teste 3 combina√ß√µes diferentes de hiperpar√¢metros (camadas, taxa de aprendizado, etc.).
3. Avalie o desempenho com acur√°cia e matriz de confus√£o.
4. Discuta o impacto dos hiperpar√¢metros escolhidos.

---

## üß† Desafio Final para Casa

**T√≠tulo:** Competi√ß√£o Final ‚Äì Ajuste de Modelo Neural

### Instru√ß√µes:

1. Cada grupo escolher√° um dataset de livre escolha.
2. Implementar uma rede neural com pelo menos 2 camadas ocultas.
3. Ajustar hiperpar√¢metros manualmente ou com busca automatizada.
4. Comparar pelo menos 2 otimizadores diferentes.
5. Gerar um relat√≥rio e gr√°fico comparando desempenho.

**Avalia√ß√£o final:** modelo mais bem ajustado + clareza na apresenta√ß√£o dos resultados.

---

## ‚úÖ Conclus√£o

A aula final re√∫ne as t√©cnicas mais modernas de otimiza√ß√£o e busca de hiperpar√¢metros, essenciais para alcan√ßar redes neurais realmente eficazes em aplica√ß√µes pr√°ticas. Encerra-se assim o ciclo de fundamentos, pr√°tica e refino de modelos em aprendizado profundo.
