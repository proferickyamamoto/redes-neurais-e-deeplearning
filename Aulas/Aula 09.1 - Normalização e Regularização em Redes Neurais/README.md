# üß† Aula 09.1: Normaliza√ß√£o e Regulariza√ß√£o em Redes Neurais

## üéØ Objetivo
Compreender as t√©cnicas de **normaliza√ß√£o** e **regulariza√ß√£o** em redes neurais, suas motiva√ß√µes, aplica√ß√µes pr√°ticas e impacto no desempenho de modelos MLP.

---

## üìö Conte√∫do Program√°tico

### 1. Por que normalizar os dados?
- Facilita a converg√™ncia do gradiente descendente.
- Evita que vari√°veis com escalas diferentes dominem o aprendizado.
- Torna o treinamento mais est√°vel e r√°pido.

### T√©cnicas de Normaliza√ß√£o
- **Min-Max Scaling**: escala os dados para o intervalo [0, 1].
- **Z-score (Padroniza√ß√£o)**: transforma os dados para m√©dia 0 e desvio padr√£o 1.

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Exemplo:
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

### 2. Regulariza√ß√£o
- Previne o **overfitting**, ou seja, quando a rede aprende demais os dados de treino.
- Adiciona uma penaliza√ß√£o ao erro para evitar pesos muito grandes.

### T√©cnicas:
- **L1 Regularization (Lasso)**: soma dos valores absolutos dos pesos.
- **L2 Regularization (Ridge)**: soma dos quadrados dos pesos (mais comum em redes neurais).

### Exemplo com MLPClassifier:
```python
from sklearn.neural_network import MLPClassifier

modelo = MLPClassifier(hidden_layer_sizes=(10,), alpha=0.01, max_iter=1000)
# alpha √© o par√¢metro de regulariza√ß√£o L2
```

---

## üí° Impactos Pr√°ticos
- Melhora a **generaliza√ß√£o** do modelo.
- Evita oscila√ß√µes nos gradientes.
- Pode exigir ajuste fino com valida√ß√£o cruzada.

---

## üß™ Atividade em Sala

**T√≠tulo:** Comparando redes com e sem normaliza√ß√£o e regulariza√ß√£o

### Instru√ß√µes:
1. Escolha qualquer dataset (como o Iris ou OpenML).
2. Treine um MLP com e sem normaliza√ß√£o dos dados.
3. Treine com e sem o par√¢metro `alpha`.
4. Compare as acur√°cias e o comportamento da rede (n√∫mero de √©pocas, estabilidade).

---

## üß† Desafio para Casa

**T√≠tulo:** Otimizando redes com regulariza√ß√£o

### Instru√ß√µes:
1. Escolha um dataset do OpenML com pelo menos 100 amostras.
2. Aplique diferentes valores de `alpha` (ex: 0.0001, 0.01, 0.1).
3. Crie gr√°ficos comparando a acur√°cia de valida√ß√£o para cada valor.
4. Apresente suas conclus√µes.

---

## ‚úÖ Conclus√£o

A **normaliza√ß√£o** e **regulariza√ß√£o** s√£o t√©cnicas fundamentais para tornar redes neurais mais eficientes, confi√°veis e robustas. Elas evitam erros comuns e melhoram o desempenho de forma significativa.

