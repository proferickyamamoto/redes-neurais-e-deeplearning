# Aula 14.2 â€“ Redes Neurais Recorrentes (RNNs) e LSTM

## ğŸ¯ Objetivo

Compreender o funcionamento das Redes Neurais Recorrentes (RNNs) e suas variantes mais utilizadas, como LSTM (Long Short-Term Memory), para modelagem de dados sequenciais e temporais, como sÃ©ries de tempo, linguagem natural e sinais biomÃ©dicos.

---

## ğŸ“˜ Parte 1 â€“ Teoria das RNNs e LSTMs

As Redes Neurais Recorrentes (RNNs) foram projetadas para lidar com dados sequenciais, onde a ordem dos elementos Ã© relevante. Diferente das redes feedforward, as RNNs possuem conexÃµes recorrentes que permitem manter uma **memÃ³ria do estado anterior**, possibilitando o aprendizado de dependÃªncias temporais. Isso as torna adequadas para tarefas como previsÃ£o de sÃ©ries temporais, reconhecimento de fala e processamento de linguagem natural \[1].

Entretanto, as RNNs tradicionais enfrentam problemas de **desvanecimento e explosÃ£o do gradiente**, dificultando o aprendizado de dependÃªncias de longo prazo. Para resolver essas limitaÃ§Ãµes, foram propostas variantes como **LSTM (Long Short-Term Memory)** \[2] e **GRU (Gated Recurrent Unit)** \[3].

<img width="1400" height="544" alt="image" src="https://github.com/user-attachments/assets/3f374388-a519-4089-960a-f1c71e259e78" />


As LSTMs utilizam um conjunto de portas (input, forget e output gates) que regulam o fluxo de informaÃ§Ã£o e permitem o aprendizado de dependÃªncias longas de forma estÃ¡vel. Esse mecanismo tornou as LSTMs o padrÃ£o de fato em muitas aplicaÃ§Ãµes envolvendo sequÃªncias, incluindo traduÃ§Ã£o automÃ¡tica, anÃ¡lise de sentimentos e previsÃ£o de sinais biomÃ©dicos, como ECGs e EEGs.

**ReferÃªncias:**
\[1] Elman, J. L. (1990). Finding structure in time. *Cognitive Science*.
\[2] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural computation*.
\[3] Cho, K. et al. (2014). Learning phrase representations using RNN encoderâ€“decoder for statistical machine translation. *EMNLP*.

---

## ğŸ’» Parte 2 â€“ ImplementaÃ§Ã£o de uma RNN simples e uma LSTM

### Exemplo com sequÃªncia numÃ©rica simples

```python
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, Dense

# Criando dados sequenciais artificiais
X = np.array([[[i+j] for i in range(5)] for j in range(100)])  # 100 amostras de sequÃªncias de tamanho 5
y = np.array([j+5 for j in range(100)])  # prÃ³ximo nÃºmero da sequÃªncia

# NormalizaÃ§Ã£o
X = X / 100.0
y = y / 100.0

# RNN simples
model_rnn = Sequential([
    SimpleRNN(10, activation='tanh', input_shape=(5,1)),
    Dense(1)
])
model_rnn.compile(optimizer='adam', loss='mse')
model_rnn.fit(X, y, epochs=50, verbose=0)

# LSTM
model_lstm = Sequential([
    LSTM(10, activation='tanh', input_shape=(5,1)),
    Dense(1)
])
model_lstm.compile(optimizer='adam', loss='mse')
model_lstm.fit(X, y, epochs=50, verbose=0)

print("PrediÃ§Ã£o RNN:", model_rnn.predict(X[:1]))
print("PrediÃ§Ã£o LSTM:", model_lstm.predict(X[:1]))
```

ExplicaÃ§Ã£o: o cÃ³digo acima cria sequÃªncias de nÃºmeros inteiros e treina uma RNN simples e uma LSTM para prever o prÃ³ximo nÃºmero. Ambas conseguem aprender o padrÃ£o, mas a LSTM tende a apresentar maior estabilidade em sequÃªncias mais longas.

---

## ğŸ§ª Atividade em Sala

**TÃ­tulo:** PrevisÃ£o de sÃ©rie temporal com LSTM

### InstruÃ§Ãµes:

1. Gere uma sÃ©rie temporal simples, como uma onda seno com ruÃ­do.
2. Prepare os dados em janelas de tempo (ex: 20 passos passados para prever o prÃ³ximo).
3. Treine um modelo LSTM para prever os prÃ³ximos valores.
4. Compare a previsÃ£o com os valores reais utilizando grÃ¡ficos.

---

## ğŸ§  Desafio para Casa

**TÃ­tulo:** ClassificaÃ§Ã£o de Sentimentos com LSTM

### InstruÃ§Ãµes:

1. Utilize o dataset IMDb disponÃ­vel no Keras (`keras.datasets.imdb`).
2. PrÃ©-processe os dados (padding das sequÃªncias).
3. Treine uma LSTM para classificar crÃ­ticas como positivas ou negativas.
4. Avalie a acurÃ¡cia no conjunto de teste.

---

## âœ… ConclusÃ£o

As RNNs e suas variantes, como as LSTMs, revolucionaram a modelagem de dados sequenciais ao possibilitar o aprendizado de dependÃªncias temporais de curto e longo prazo. Sua aplicaÃ§Ã£o Ã© ampla, cobrindo desde a previsÃ£o de sÃ©ries temporais atÃ© o processamento de linguagem natural, e estabelecendo as bases para arquiteturas mais avanÃ§adas que veremos adiante, como Transformers.
