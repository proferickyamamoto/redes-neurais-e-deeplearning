# üß† Aula 05.1: Introdu√ß√£o √†s Redes Neurais Artificiais (RNAs)

As Redes Neurais Artificiais s√£o modelos computacionais inspirados na estrutura do c√©rebro humano. Elas s√£o compostas por unidades chamadas **neur√¥nios artificiais**, que est√£o interconectadas por **pesos sin√°pticos**. Esses modelos s√£o capazes de **aprender padr√µes a partir de dados** e fazer previs√µes, classifica√ß√µes ou regress√µes.

---

## üß¨ Inspira√ß√£o Biol√≥gica

| Neur√¥nio Biol√≥gico     | Neur√¥nio Artificial           |
|-------------------------|-------------------------------|
| Dendritos: recebem sinais | Entradas (features do dado)    |
| Corpo celular: processa   | Soma ponderada das entradas   |
| Ax√¥nio: envia sinal       | Sa√≠da do neur√¥nio             |

---

## ‚öôÔ∏è Arquitetura de uma RNA

Uma rede neural √© composta por camadas:

- **Camada de entrada**: recebe os dados.
- **Camadas ocultas**: fazem transforma√ß√µes nos dados.
- **Camada de sa√≠da**: gera o resultado final.

Cada conex√£o entre neur√¥nios possui um **peso**, que √© ajustado durante o treinamento.

### üßÆ Modelo Matem√°tico de um Neur√¥nio

\[ u = \sum_{i=1}^{n} x_i w_i \]

\[ y = f(u + b) \]

Onde:
- \( x_i \): entrada
- \( w_i \): peso associado √† entrada
- \( b \): vi√©s (bias)
- \( f \): fun√ß√£o de ativa√ß√£o (ex.: sigmoide, ReLU)

---

## üîß Fun√ß√µes de Ativa√ß√£o

- **Step Function (limiar):** bin√°ria, simples, usada no Perceptron original.
- **Sigmoide:** \( f(x) = \frac{1}{1 + e^{-x}} \)
- **ReLU (Rectified Linear Unit):** \( f(x) = \max(0, x) \)

---

## üîÑ Etapa de Feedforward

A propaga√ß√£o direta (feedforward) consiste em:
1. Receber as entradas na camada de entrada.
2. Multiplicar pelas conex√µes com os pesos.
3. Somar os resultados.
4. Aplicar a fun√ß√£o de ativa√ß√£o.
5. Enviar a sa√≠da para a pr√≥xima camada.

Essa etapa ocorre **da entrada at√© a sa√≠da final**, sem ajuste de pesos (sem aprendizado).

---

## üíª Implementa√ß√£o em Python (Feedforward)

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.W1 = np.random.randn(input_size, hidden_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size)
        self.b2 = np.zeros((1, output_size))

    def feedforward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.output = sigmoid(self.z2)
        return self.output

# Exemplo de uso:
X = np.array([[0, 1]])  # entrada bin√°ria
nn = NeuralNetwork(input_size=2, hidden_size=3, output_size=1)
saida = nn.feedforward(X)
print("Sa√≠da da rede:", saida)
```

---

## üß™ Atividades em Sala

### ‚úîÔ∏è Atividade 1 ‚Äì Entendendo o Perceptron
- Construa, com papel e caneta, o modelo de um perceptron com 2 entradas.
- Atribua valores fict√≠cios de entrada e pesos.
- Calcule manualmente a sa√≠da usando a fun√ß√£o degrau.

### ‚úîÔ∏è Atividade 2 ‚Äì Teste da Rede em Python
- Execute o c√≥digo da rede neural em Python (fornecido acima).
- Altere as entradas e analise como a sa√≠da da rede muda.
- Documente os testes em uma tabela com entrada ‚Üí sa√≠da.

### ‚úîÔ∏è Atividade 3 ‚Äì Visualiza√ß√£o da Arquitetura
- Em grupos, desenhem a arquitetura da rede neural do exemplo com:
  - Camada de entrada (2 neur√¥nios)
  - Camada oculta (3 neur√¥nios)
  - Camada de sa√≠da (1 neur√¥nio)
- Identifiquem as conex√µes e indiquem como o feedforward ocorre.

---

## üß† Desafio para Casa

**T√≠tulo:** Criando um Classificador de Portas L√≥gicas com Feedforward

### Objetivo:
Construir uma rede neural simples (sem backpropagation) que classifique os resultados das portas **AND**, **OR** e **XOR** com base no comportamento observado no feedforward.

### Instru√ß√µes:
1. Crie um script Python com a classe `NeuralNetwork`.
2. Insira os conjuntos de entrada e sa√≠da correspondentes a AND, OR e XOR.
3. Execute o `feedforward()` com diferentes pesos e analise se a rede se comporta corretamente.
4. Explique por que a rede consegue ou n√£o resolver o problema da porta XOR.

### Entrega:
- C√≥digo comentado (Python `.ipynb` ou `.py`)
- Tabela de testes
- Pequeno relat√≥rio (at√© 1 p√°gina)

---

## ‚úÖ Conclus√£o

A etapa de **feedforward** √© a base do funcionamento das RNAs, sendo essencial para propagar os dados pelas camadas. O **aprendizado** da rede ocorre posteriormente, com o uso do algoritmo de **backpropagation**, que ajusta os pesos com base no erro da sa√≠da.

Na pr√≥xima aula, veremos como as RNAs **aprendem**, minimizando o erro com **fun√ß√£o custo** e **gradiente descendente**.


