# üß† Introdu√ß√£o √†s Redes Neurais Artificiais (RNAs)

As Redes Neurais Artificiais s√£o modelos computacionais inspirados na estrutura do c√©rebro humano. Elas s√£o compostas por unidades chamadas **neur√¥nios artificiais**, que est√£o interconectadas por **pesos sin√°pticos**. Esses modelos s√£o capazes de **aprender padr√µes a partir de dados** e fazer previs√µes, classifica√ß√µes ou regress√µes.

---

## üß¨ Inspira√ß√£o Biol√≥gica

| Neur√¥nio Biol√≥gico     | Neur√¥nio Artificial           |
|-------------------------|-------------------------------|
| Dendritos: recebem sinais | Entradas (features do dado)    |
| Corpo celular: processa   | Soma ponderada das entradas   |
| Ax√¥nio: envia sinal       | Sa√≠da do neur√¥nio             |

---

## ‚öôÔ∏è Arquitetura de uma RNA

Uma rede neural √© composta por camadas:

- **Camada de entrada**: recebe os dados.
- **Camadas ocultas**: fazem transforma√ß√µes nos dados.
- **Camada de sa√≠da**: gera o resultado final.

Cada conex√£o entre neur√¥nios possui um **peso**, que √© ajustado durante o treinamento.

### üßÆ Modelo Matem√°tico de um Neur√¥nio

\$$ u = \sum_{i=1}^{n} x_i w_i \$$

\$$ y = f(u + b) \$$

Onde:
- \$$( x_i )\$$: entrada
- \$$( w_i )\$$: peso associado √† entrada
- \$$( b )\$$: vi√©s (bias)
- \$$( f )\$$: fun√ß√£o de ativa√ß√£o (ex.: sigmoide, ReLU)

---

## üîß Fun√ß√µes de Ativa√ß√£o

- **Step Function (limiar):** bin√°ria, simples, usada no Perceptron original.
- **Sigmoide:** \$$[ f(x) = \frac{1}{1 + e^{-x}} ]\$$
- **ReLU (Rectified Linear Unit):** \$$[ f(x) = \max(0, x) ]\$$

---

## üîÑ Etapa de Feedforward

A propaga√ß√£o direta (feedforward) consiste em:
1. Receber as entradas na camada de entrada.
2. Multiplicar pelas conex√µes com os pesos.
3. Somar os resultados.
4. Aplicar a fun√ß√£o de ativa√ß√£o.
5. Enviar a sa√≠da para a pr√≥xima camada.

Essa etapa ocorre **da entrada at√© a sa√≠da final**, sem ajuste de pesos (sem aprendizado).

---

## üíæ Redes Neurais com Mem√≥ria RAM (RAM-based Neural Networks)

As **Redes Neurais RAM** s√£o arquiteturas alternativas √†s redes convencionais baseadas em pesos. Inspiradas nos conceitos de **mem√≥ria de acesso aleat√≥rio (RAM)**, essas redes armazenam padr√µes diretamente na mem√≥ria e os acessam por endere√ßamento.

### Caracter√≠sticas:
- N√£o utilizam pesos; os neur√¥nios s√£o implementados com c√©lulas de mem√≥ria.
- A entrada √© usada como endere√ßo para recuperar o conte√∫do armazenado.
- Aprendizado simples: armazenar a presen√ßa de padr√µes na mem√≥ria.

### Exemplos:
- **WiSARD**: Rede baseada em RAM com neur√¥nios que armazenam padr√µes bin√°rios.
- **PLN (Probabilistic Logic Networks)**: vers√£o probabil√≠stica que lida com incertezas.

### Vantagens:
- Alta velocidade de aprendizado (one-shot learning).
- Simplicidade de implementa√ß√£o em hardware.

### Limita√ß√µes:
- Pouca generaliza√ß√£o em entradas n√£o vistas.
- Pode exigir grande quantidade de mem√≥ria em problemas complexos.

As RNAs RAM s√£o eficazes em aplica√ß√µes de reconhecimento de padr√µes bin√°rios, como reconhecimento de caracteres, biometria e detec√ß√£o de padr√µes visuais simples.

---

## üíª Implementa√ß√£o em Python (Feedforward)

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def step_function(x):
    return np.where(x >= 0, 1, 0)

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):
        self.W1 = np.random.randn(input_size, hidden_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size)
        self.b2 = np.zeros((1, output_size))
        self.activation = activation

    def activate(self, x):
        if self.activation == 'sigmoid':
            return sigmoid(x)
        elif self.activation == 'step':
            return step_function(x)
        else:
            raise ValueError("Fun√ß√£o de ativa√ß√£o desconhecida")

    def feedforward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.activate(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.output = self.activate(self.z2)
        return self.output

# Exemplo de uso com fun√ß√£o degrau:
X = np.array([[0, 1]])  # entrada bin√°ria
nn = NeuralNetwork(input_size=2, hidden_size=3, output_size=1, activation='step')
saida = nn.feedforward(X)
print("Sa√≠da da rede:", saida)
```

---

## üß™ Atividades em Sala

### ‚úîÔ∏è Atividade 1 ‚Äì Entendendo o Perceptron
- Construa, com papel e caneta, o modelo de um perceptron com 2 entradas.
- Atribua valores fict√≠cios de entrada e pesos.
- Calcule manualmente a sa√≠da usando a fun√ß√£o degrau.

### ‚úîÔ∏è Atividade 2 ‚Äì Teste da Rede em Python
- Execute o c√≥digo da rede neural em Python (fornecido acima).
- Altere as entradas e analise como a sa√≠da da rede muda.
- Documente os testes em uma tabela com entrada ‚Üí sa√≠da.

### ‚úîÔ∏è Atividade 3 ‚Äì Visualiza√ß√£o da Arquitetura
- Em grupos, desenhem a arquitetura da rede neural do exemplo com:
  - Camada de entrada (2 neur√¥nios)
  - Camada oculta (3 neur√¥nios)
  - Camada de sa√≠da (1 neur√¥nio)
- Identifiquem as conex√µes e indiquem como o feedforward ocorre.

---

## üß† Desafio para Casa

**T√≠tulo:** Criando um Classificador de Portas L√≥gicas com Feedforward

### Objetivo:
Construir uma rede neural simples (sem backpropagation) que classifique os resultados das portas **AND**, **OR** e **XOR** com base no comportamento observado no feedforward.

### Instru√ß√µes:
1. Crie um script Python com a classe `NeuralNetwork`.
2. Insira os conjuntos de entrada e sa√≠da correspondentes a AND, OR e XOR.
3. Execute o `feedforward()` com diferentes pesos e analise se a rede se comporta corretamente.
4. Explique por que a rede consegue ou n√£o resolver o problema da porta XOR.

### Entrega:
- C√≥digo comentado (Python `.ipynb` ou `.py`)
- Tabela de testes
- Pequeno relat√≥rio (at√© 1 p√°gina)

---

## ‚úÖ Conclus√£o

A etapa de **feedforward** √© a base do funcionamento das RNAs, sendo essencial para propagar os dados pelas camadas. O **aprendizado** da rede ocorre posteriormente, com o uso do algoritmo de **backpropagation**, que ajusta os pesos com base no erro da sa√≠da.

Na pr√≥xima aula, veremos como as RNAs **aprendem**, minimizando o erro com **fun√ß√£o custo** e **gradiente descendente**.

