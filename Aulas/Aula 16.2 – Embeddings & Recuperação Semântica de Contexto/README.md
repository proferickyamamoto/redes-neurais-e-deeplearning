# Aula 16.2 â€“ Embeddings & RecuperaÃ§Ã£o SemÃ¢ntica de Contexto

## ğŸ¯ Objetivo

Nesta semana os alunos vÃ£o entender:

* O que sÃ£o embeddings de frase e porque sÃ£o Ãºteis para recuperar contexto relevante.
* Como construir uma base vetorial de documentos (conhecimento) e consultÃ¡-la eficientemente via busca semÃ¢ntica (ex: com FAISS).
* Como usar essa base vetorial dentro do pipeline de Q&A para melhorar as respostas do modelo generativo.

---

## ğŸ“˜ Parte 1 â€“ Teoria: embeddings de frase e similaridade semÃ¢ntica

### ğŸ”¹ O que sÃ£o embeddings de frase?

Embeddings de frase (sentence embeddings) transformam uma frase inteira em um vetor numÃ©rico fixo que captura seu significado semÃ¢ntico (contexto, sinÃ´nimos, intenÃ§Ãµes). Diferentemente de embeddings de palavras (Word2Vec, GloVe), eles permitem comparar sentenÃ§as completas. ([Wikipedia][1])

Por exemplo, as frases â€œO gato estÃ¡ dormindoâ€ e â€œO felino repousaâ€ devem gerar embeddings prÃ³ximos, porque expressam ideias semelhantes.

Modelos como **Sentence-BERT (SBERT)** otimizam embeddings de frase usando redes siamesas (Siamese Networks) para treinar de modo que frases semanticamente prÃ³ximas fiquem prÃ³ximas no espaÃ§o vetorial. ([sbert.net][2])

### ğŸ”¹ MÃ©tricas de similaridade

Para comparar embeddings, usamos mÃ©tricas como:

* **DistÃ¢ncia Euclidiana (L2)**: mede a â€œdistÃ¢ncia padrÃ£oâ€ entre vetores.
* **Produto interno (dot product) / Similaridade cosseno**: considera direÃ§Ã£o dos vetores; Ãºtil para embeddings normalizados.

A similaridade ajuda a encontrar quais documentos sÃ£o semanticamente mais prÃ³ximos Ã  pergunta.

### ğŸ”¹ Biblioteca FAISS para busca eficiente

Quando temos muitos embeddings (milhares ou milhÃµes), buscar exaustivamente Ã© caro. A FAISS (Facebook AI Similarity Search) Ã© uma biblioteca otimizada em C++ com bindings Python para indexar vetores densos e realizar buscas de vizinhos mais prÃ³ximos de forma eficiente. ([Engineering at Meta][3])

Por exemplo, vocÃª pode adicionar embeddings ao Ã­ndice FAISS e, dado um vetor de consulta, recuperar os k documentos mais similares em tempo rÃ¡pido.

FAISS suporta diferentes tipos de Ã­ndices (flat, IVFFlat, HNSW, etc.) para escalabilidade e precisÃ£o. ([Medium][4])

---

## ğŸ’» Parte 2 â€“ ImplementaÃ§Ã£o prÃ¡tica: pipeline de vetorizaÃ§Ã£o + indexaÃ§Ã£o + busca

Aqui vamos construir:

1. Uma base de documentos (contextos) do conhecimento.
2. Gerar embeddings de frase para cada documento.
3. Construir Ã­ndice FAISS para busca.
4. Dada uma pergunta, embedar a pergunta e recuperar os documentos mais relevantes.

### ğŸ”¹ InstalaÃ§Ã£o de dependÃªncias

```bash
pip install sentence-transformers faiss-cpu
```

(O `faiss-cpu` Ã© a versÃ£o sem GPU. Se vocÃª tiver GPU, use `faiss-gpu`.)

### ğŸ”¹ CÃ³digo: vetorizaÃ§Ã£o e indexaÃ§Ã£o

```python
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# 1. Definir documentos do conhecimento
docs = [
    "A IA generativa cria novos conteÃºdos a partir de dados existentes.",
    "Redes neurais aprendem padrÃµes complexos dos dados.",
    "Embeddings transformam texto em vetores numÃ©ricos.",
    "Transformers sÃ£o a arquitetura central em modelos de linguagem.",
    "Busca semÃ¢ntica melhora a recuperaÃ§Ã£o de contexto relevante."
]

# 2. Gerar embeddings com modelo prÃ©-treinado
model_embed = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model_embed.encode(docs, convert_to_numpy=True)
# embeddings.shape = (n_docs, dim_embedding)

# 3. Construir Ã­ndice FAISS
dim = embeddings.shape[1]
index = faiss.IndexFlatIP(dim)  # Ã­ndice para similaridade por produto interno
faiss.normalize_L2(embeddings)  # normalizar vetores (para usar produto interno como cosseno)
index.add(embeddings)
```

### ğŸ”¹ CÃ³digo: buscar contexto relevante para uma pergunta

```python
def retrieve_top_k(question, k=2):
    q_emb = model_embed.encode([question], convert_to_numpy=True)
    faiss.normalize_L2(q_emb)
    D, I = index.search(q_emb, k)  # D: distÃ¢ncias (scores), I: Ã­ndices dos documentos
    return I[0], D[0]

# Exemplo:
q = "O que Ã© IA generativa?"
idxs, scores = retrieve_top_k(q, k=2)
print("Documentos mais relevantes:")
for i, score in zip(idxs, scores):
    print(f"- {docs[i]} (score {score:.4f})")
```

ExplicaÃ§Ãµes:

* `IndexFlatIP(dim)` cria um Ã­ndice que compara vetores via produto interno (Ãºtil quando embeddings jÃ¡ estÃ£o normalizados).
* `normalize_L2(...)` faz a normalizaÃ§Ã£o para que dot product equivalha Ã  similaridade cosseno.
* `index.search(...)` retorna os k documentos mais similares ao vetor da pergunta.

### ğŸ”¹ IntegraÃ§Ã£o com pipeline de Q&A

Depois de recuperar os documentos mais relevantes (por exemplo, os top-2), vocÃª pode concatenÃ¡-los como `context` para o modelo de perguntas-respostas ou modelo generativo:

```python
context = " ".join(docs[i] for i in idxs)  # juntar os documentos recuperados
# EntÃ£o alimentamos o pipeline de Q&A ou modelo generativo com esse contexto
```

Isso permite que o modelo tenha acesso ao trecho mais relevante do â€œconhecimentoâ€ para responder.

---

## ğŸ§  Parte 3 â€“ Exemplos e analogias para entendimento

### ğŸ§© Analogia de busca semÃ¢ntica

Imagine que cada documento Ã© uma **estrela** em um mapa galÃ¡ctico (o espaÃ§o vetorial).
Quando vocÃª faz uma pergunta, vocÃª gera uma **estrela consulta** nesse mapa.
A FAISS ajuda vocÃª a encontrar qual estrela/documento estÃ¡ mais prÃ³xima da sua estrela de pergunta â€” essas estrelas prÃ³ximas representam documentos semanticamente mais parecidos.

### âš–ï¸ Por que FAISS Ã© mais eficiente?

Se vocÃª tivesse que comparar manualmente cada documento da base com a pergunta (busca exaustiva), isso custaria muito tempo.
FAISS usa estruturas de Ã­ndice (listas invertidas, quantizaÃ§Ã£o, grafos de vizinhanÃ§a) para **reduzir drasticamente o nÃºmero de comparaÃ§Ãµes**, suportando buscas rÃ¡pidas. ([Medium][4])

---

## ğŸ§ª Atividade em Sala â€“ RecuperaÃ§Ã£o Contextual

**TÃ­tulo:** Construindo e utilizando uma base vetorial de contexto

### InstruÃ§Ãµes:

1. Use a base de documentos `docs` ou crie sua prÃ³pria base (5 a 10 parÃ¡grafos).
2. Gere embeddings de frase usando **SentenceTransformer** (ex: `all-MiniLM-L6-v2`).
3. Indexe com FAISS e normalize os embeddings.
4. Para 3 perguntas distintas, recupere os top-k documentos relevantes.
5. Exiba os documentos, scores e discuta se as escolhas fazem sentido.

### Resultados esperados:

* Pergunta â€œO que Ã© IA generativa?â€ â†’ deve recuperar o documento â€œA IA generativa cria novos conteÃºdosâ€¦â€ com score alto.
* Pergunta â€œComo funcionam embeddings?â€ â†’ deve recuperar â€œEmbeddings transformam texto em vetoresâ€¦â€ e possivelmente â€œRedes neurais aprendem â€¦â€
* Pergunta â€œO que sÃ£o transformers?â€ â†’ deve recuperar â€œTransformers sÃ£o a arquitetura centralâ€¦â€

Explique por que a busca deu esses resultados.

---

## ğŸ“„ Desafio para Casa â€“ Vetorizar base maior + testar pipeline completo

**TÃ­tulo:** Q&A com base de contexto dinÃ¢mica

### InstruÃ§Ãµes:

1. ReÃºna 20â€“30 textos sobre um tema escolhido (ex: â€œRedes Neurais e Deep Learningâ€).
2. Gere embeddings e construa Ã­ndice FAISS (ou use variante aproximada).
3. Para cada pergunta da turma (ex: 5 perguntas), recupere top-k documentos.
4. Use o modelo de perguntas-respostas (da Semana 1 ou modelo generativo) com esse contexto recuperado.
5. Compare: respostas com contexto recuperado vs. respostas com contexto fixo (sem indexaÃ§Ã£o).
6. Entregue notebook + relatÃ³rio tÃ©cnico (incluindo nomes/RMs).

---

## ğŸ“š ReferÃªncias Relevantes

* Sentence Transformers: biblioteca prÃ¡tica para embeddings de frase. ([sbert.net][2])
* FAISS: biblioteca eficiente para busca de similaridade vetorial. ([Engineering at Meta][3])
* â€œTop 4 Sentence Embedding Techniquesâ€ â€” introduÃ§Ã£o a embeddings de frase. ([Analytics Vidhya][5])


[1]: https://en.wikipedia.org/wiki/Sentence_embedding?utm_source=chatgpt.com "Sentence embedding"
[2]: https://sbert.net/?utm_source=chatgpt.com "SentenceTransformers Documentation â€” Sentence Transformers ..."
[3]: https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/?utm_source=chatgpt.com "Faiss: A library for efficient similarity search - Engineering at Meta"
[4]: https://medium.com/%40devbytes/similarity-search-with-faiss-a-practical-guide-to-efficient-indexing-and-retrieval-e99dd0e55e8c?utm_source=chatgpt.com "Similarity Search with FAISS: A Practical Guide to Efficient Indexing ..."
[5]: https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/?utm_source=chatgpt.com "Top 4 Sentence Embedding Techniques using Python"
